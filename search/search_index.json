{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kubernetes Lab Setup","text":""},{"location":"#what-is-kubernetes-lab-setup","title":"What is Kubernetes Lab Setup ?","text":"<p>Setting up Kubernetes locally on your Computer, is easy.  Kubernetes Lab Setup aims to provide a setup that is similar to a production environment.</p> <p>This lab setup does not go into the Kubernetes direclty, but with this setup testing out applications on Kubernetes becomes so much easier. </p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>In order to understand the steps in Kubernetes Lab Setup, the following knowledge is required. * Basic Linux knowledge - You should be able to connect and navigate a Linux environment. * Basic Hyper-V - Be able to enable Hyper-V and be familiar with the steps to create Virtual Machines * PowerShell - Be able to use PowerShell ISE.</p>"},{"location":"#how-to-use-kubernetes-lab-setup","title":"How to use Kubernetes Lab Setup ?","text":"<p>The steps to deploy follow a certain sequence. Be sure to follow the sequence as described here. When I try some new guide, I sometimes change names. Until you are aware of the impact of the changes , don't change anyting. </p> <p>Being a Lab setup, comes with the need to destroy and rebuild , with the steps in Kubernetes Lab Setup the rebuild is much more easier and faster. </p>"},{"location":"ReadMe/","title":"Kubernetes Lab Setup","text":"<p>This repository contains the relevant steps to setup</p> <ul> <li>1 Alpine OS Management Server</li> <li>1 single node  Lightweight Kubernetes ( k3s)</li> <li>1 High Available Cluster Lightweight Kubernetes ( k3s)</li> </ul> <p>The objective of this lab is to be able to rebuild the clusters very quickly as this is for testing services and applications. </p> <p>Files required for the setup can be extracted from the git repository  https://github.com/belajarpowershell/kubernetes-lab</p> <p>The guide is designed such that , someone with basic knowledge of Ubuntu , can setup the lab succesfully. Basic knowledge of Ubuntu only requires connecting, navigating and editing files .</p> <p>Follow the sequence number in the files , this will ensure that all the required services are setup.</p> <p>Here is a high level on the tasks required .</p> <p>Operating System Setup <code>100-alpine1</code></p> <ul> <li>Clone the repository to the Hyper V host. (step <code>003-clone-repository</code>)</li> <li>Create the Virtual machines required using the PowerShell script. ( step <code>004-Hyper-V-VM-creation</code> )</li> <li>Setup Alpine Linux on VM <code>alpine1</code> .(step <code>100-alpine1-setup</code>)</li> <li>Connect to <code>alpine1</code> via ssh (step <code>101-ssh-to-alpine1</code> )</li> <li>Clone the repository on <code>alpine1</code> ( step 102-Clone-repository)</li> <li>Setup on <code>alpine1</code> the various services required . (step <code>103-setup-using-ansible</code>)</li> <li>Install Ubuntu OS on the remaining Virtual Machines ( loadbalancer,master\u00bd/3,worker\u00bd/3,singlenode) (step<code>113-generate-user-data-multipleVM</code> )</li> </ul> <p>Lightweight k3s  setup <code>200-kubernetes</code></p> <ul> <li>Summary of steps (step  <code>200ReadMe</code>)</li> <li>Create and setup for ssh-key for authentication (step <code>201-SSH-keys-setup</code>)</li> <li>Setup Ansible </li> <li>Install the all dependencies and k3s on loadbalancer,master\u00bd/3,worker\u00bd/3,singlenode (step <code>202-Setup-k3s-with-Ansible</code>)</li> </ul>"},{"location":"kubernetes-lab-setup/000-Prerequisites/001-Hyper-V-Host-Specification/","title":"Hyper-V Specificatios","text":"<p>In this Lab setup The Hyper V host server specifications are as follows</p> <ol> <li>Intel i7-8700 3.2GHz CPU</li> <li>32GB of Memory</li> <li>At least 1 Terabytes of Hard Disk</li> </ol> <p>As a reference with all the Virtual Machines Running the following was the utilization 1. CPU &lt;50% utilization 2. Memory ~ 64% </p> <p>This is to provide a perspective on the Host CPU specifications for your Lab setup.</p>"},{"location":"kubernetes-lab-setup/000-Prerequisites/002-High-Level-node-setup/","title":"High Level Design","text":"hostname IP address alpine1 192.168.100.1/24 Hyper-V-Host 192.168.100.2/24 xsinglenode 192.168.100.199/24 loadbalancer 192.168.100.201/24 master1 192.168.100.202/24 master2 192.168.100.203/24 master3 192.168.100.204/24 worker1 192.168.100.205/24 worker2 192.168.100.206/24 worker3 192.168.100.207/24 metallb 192.168.100.208-215 <p><code>alpine1</code> functions for the devices in the subnet <code>192.168.100.0/24</code> as  - DHCP server  - Internet Router This server has 2 NIC one for the Internet access and another on the subnet <code>192.168.100.0/24</code> - DNS - NGINX web server - NFS server - TFTP server - Hosting the various files required for the Ubuntu and Kubernetese setup</p> <p><code>xsinglenode</code> Is a single node implementation of Kubernetes.</p> <p><code>loadbalancer</code> functions as the entry point to the Master nodes, performing the High Availability fucntions for the multiple Master nodes.</p> <p><code>master1/2/3</code> This is the Kubernetes Control plane. Acting as a High Availability setup that allows for 1 Master node failure at any time.</p> <p><code>worker1/2/3</code> The Kubernetes workloads are deployed on these nodes. 3 nodes allow for some load balancing in the event nodes need to be taken down for maintenance purposes.</p> <p><code>metallb</code> The Ingress requires a set of IP addresses to cater for incoming traffic to the Kubernetes cluster. These IP ranges are reserved for this functionality.</p>"},{"location":"kubernetes-lab-setup/000-Prerequisites/002-High-Level-node-setup/#high-level-network-diagram","title":"High Level Network Diagram","text":"<p>HYPER-V Network connectivity All the VM's will use <code>alpine1</code> as the gateway to the internet. Apart from the gateway function, <code>alpine1</code> also acts as performs several other functions. </p> <pre><code>flowchart TD\nsubgraph HYPER-V Network connectivity\nInternet o--o \n    newLines[\"`alpine1\n    DHCP\n    Router\n    DNS\n    NGINX\n    NFS\n    TFTP`\"]\nnewLines[\"`**alpine1**\nDHCP \nRouter\nDNS\nNGINX\nNFS\nTFTP`\"]  o--o alpine1\nalpine1 o--o xsinglenode;\nalpine1 o--o loadbalancer;\nalpine1 o--o master1;\nalpine1 o--o master2;\nalpine1 o--o master3;\nalpine1 o--o worker1;\nalpine1 o--o worker2;\n\nalpine1--&gt;worker3;\nend</code></pre>"},{"location":"kubernetes-lab-setup/000-Prerequisites/002-vi-basics/","title":"VI editor basics","text":"<p>VI is a terminal editor tool, there are many other editors but is available on both Alpine and Ubuntu . And most importantly I am used to VI. </p> <p>If you are a beginner, there are some basic commands you will need to know. </p> Action command notes Create/ Edit file vi filename.txt run this from the same folder the file is located. If its in a different folder, use the relevant path to the file insert mode <code>i</code> When you open a file with VI, you cannot edit the file. To edit enter <code>i</code> to insert in command mode. You now can type to make changes. save and exit <code>esc</code> :x Once edits are complete, press <code>esc</code> to enter the command mode. The <code>:x</code> to save and exit exit without saving <code>esc</code> <code>:q!</code> To not save the file, press <code>esc</code> to enter the command mode. The <code>:q!</code> to exit Copy selection left mouse click to copy , right mouse click to paste Left click and drag selection, right click to paste <p>There are many more that you can use, but with these basics you should be able to perform the tasks required in this Lab setup guide.</p>"},{"location":"kubernetes-lab-setup/000-Prerequisites/003-clone-repository/","title":"Clone Repository","text":"<p>The scripts required to perform the various tasks are located in the git repository  https://github.com/belajarpowershell/kubernetes-lab</p> <p>You will need to have GIT installed Install Git  </p> <p>On Windows 11 you can open a <code>Terminal</code>  that can run the `git' commands. </p> <p>The following command will create a folder <code>kubernetes-lab</code> and download the files.</p> <pre><code># change to the Drive you want to store the files before executing this.\ngit clone https://github.com/belajarpowershell/kubernetes-lab.git\n</code></pre> <p></p> <p></p> <p>The files can then be found in ( in this example its the c: drive it might be different for you.)</p> <pre><code>c:\\kubernetes-lab\n</code></pre>"},{"location":"kubernetes-lab-setup/000-Prerequisites/004-Hyper-V-VM-creation/","title":"Hyper-V Virtual Machine creation","text":"<p>As part of a Lab setup, I expect to create and destroy the VM's regularly. To reduce the pain of recreating manually, I use a PowerShell script to create the Hyper-V VM's with certain specifications.</p> <p>To use this script, you must have already have a working Hyper-V. You can refer to this link Enable Hyper-V on Windows 11</p> <p>To use this script:- - run <code>Powershell ISE</code> as Administrator. ( Right click <code>PowerShell ISE</code> select <code>Run as Administrator</code>)</p> <ul> <li>Open the file <code>G:\\kubernetes-lab\\srv\\scripts\\001-Kubernetes-Create-HyperV-VM copy.csv</code></li> </ul> <p>Click on the <code>Green Play</code> button, this will execute the script.</p> <p>// method to access Video-- QR code?</p> <p>Here is a video that might help</p>"},{"location":"kubernetes-lab-setup/000-Prerequisites/004-Hyper-V-VM-creation/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Hyper-V VM Memory needs to be at minimum 1GB. Any lower the Ubuntu will fail to install. Wasted a few days to catch this. </li> <li>If you are using the download ISO to memory then you need to have a minimum of 4GB on the Hyper-V VM  to load the ISO to memory. </li> </ol>"},{"location":"kubernetes-lab-setup/000-Prerequisites/004-Hyper-V-VM-creation/#once-the-virtual-machines-are-created-proceed-to-100-alpine1-setup-steps","title":"Once the Virtual Machines are created proceed to <code>100-alpine1-setup</code> steps.","text":""},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/","title":"Install Alpine OS on VM <code>alpine1</code>","text":""},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#key-information","title":"Key information","text":"<pre><code>Alpine Default login\nlogin : root\npassword : blank\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#hyper-v-configuration-for-reference","title":"Hyper-V configuration for reference","text":""},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#alpine-install-iso-first-boot","title":"Alpine install ISO first boot","text":""},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#disable-ipv6-on-alpine","title":"Disable IPv6 on Alpine","text":"<p>In my setup IPv6 was not working.When downloading files IPv6 was used and delayed/errored out. If you have issues with IPv6, do disable before proceeding. </p> <p>Edit the /etc/sysctl.conf</p> <pre><code>vi /etc/sysctl.conf\n# Manualy type in the following\n# press [i] to be in insert /edit mode\n\nnet.ipv6.conf.all.disable_ipv6 = 1\n# to save and exit following key press\n# [esc] :x! [enter]\n\n# reload the configuration\nsysctl -p\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#install-alpine","title":"Install Alpine","text":"<p>After logging on at the prompt type  <pre><code>setup-alpine\n</code></pre> You will be asked a series of questions . I have listed my selection. <pre><code>Keyboard Layout : [us]\nKeyboard variant : [us]\nHostname : [alpine1]\nNetwork :\neth0: [dhcp]  # Internet connection\neth1: [192.168.100.1/24]\ngateway: [none]\npassword : [123]\nDNS Servers :\nTimezone :[Singpore]\nProxy :[none]\nntp: default [chrony]  #if it takes long to complete , make sure ipv6 is disabled\nMirror :[r] # select random from list\nsetup user : [no]\nWhich ssh server? [openssh]\nallow root ssh login: [yes]\nAllow ssh login : [yes]\nWhich disks would you like to use : [sda]\nDisk Mode How would you like to use it ? [sys]\nErase the above disks and continue? [y]\n</code></pre></p>"},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#installation-begins","title":"Installation begins.","text":"<p>Once the installation is completed you will see the notice to reboot. Before rebooting, change the boot sequence in Hyper-V for alpine1 to be Hard Drive first.</p> <p>This will ensure the reboot will be to the Hard Disk.  On the apline1 Hyper-V console. the prompt will display press [enter] <pre><code>installation complete type reboot [enter]\n</code></pre></p>"},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#setup-completed","title":"Setup completed","text":"<p>These are the credentials for newly installed alpine1 .  <pre><code>login: root\npass: 123\n#in this lab all passwords will be set with 123\n</code></pre> This screen will appear , take note of the login prompt 'alpine1'. Indicating the installation is completed.</p> <p></p>"},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#alpine-os-install-completed","title":"Alpine OS install completed","text":""},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#next-step","title":"Next step","text":"<p>We will proceed to setup alpine1 as a DHCP server </p> <p>Please continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/100-alpine1-setup/#101-alpine1-setup-dhcp-server","title":"101-alpine1-setup-DHCP-server","text":""},{"location":"kubernetes-lab-setup/100-alpine1/1000-Clone-repository%20%28copy%29/","title":"Kubernetes Lab Setup","text":"<p>This repository contains the relevant steps to setup 2  Lightweight Kubernetes ( k3s) a single node and a High Available Cluster.</p> <p>The objective of this lab is to be able to rebuild the clusters very quickly as this is for testing services and applications. </p> <p>Files required for the setup can be extracted from the git repository from the script below.</p> <p>Follow the sequence number in the files , this will ensure that all the required services are setup.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/1000-Clone-repository%20%28copy%29/#this-script-is-to-download-the-folder-srv-from-the-repository-httpsgithubcombelajarpowershellkubernetes-lab","title":"This script is to download the folder /srv from the Repository https://github.com/belajarpowershell/kubernetes-lab","text":"<pre><code># create a file download-srv.sh with below contents on `alpine1`\n# run following command to make this executable\n# chmod +x download-srv.sh\n# run the script\n# ./download-srv.sh \n\n# This script will pull the specific folder/srv from the git repository and move the contents to the /srv on alpine1.\n\n# install git \napk add git\n\n# initialize a new folder\ngit init kubernetes-lab\n# change folder\ncd kubernetes-lab\n# add git repository to folder. \ngit remote add origin https://github.com/belajarpowershell/kubernetes-lab.git\n# enable sparsecheckout\ngit config core.sparsecheckout true\n# specify folder to pull\necho \"srv/*\" &gt;&gt; .git/info/sparse-checkout\n\n# pull files locally.\ngit pull --depth=1 origin main\n\n#create folder /srv if not already existing.\n[ -d \"/srv\" ] || mkdir -p /srv &amp;&amp; echo \"Created /srv directory\" || echo \"/srv directory already exists\"\n\n\n# move files to the /srv folder.\nmv srv/* /srv\necho \" files moved to /srv \" \necho \" script completed\"\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/1000-Clone-repository%20%28copy%29/#folder-structure","title":"Folder Structure","text":"<ul> <li>/kubernetes-lab<ul> <li>k3s-lab-setup</li> <li>The server sequence of setup</li> </ul> </li> <li>scripts</li> <li>srv/<ul> <li>tftp/</li> <li>bios</li> <li>efi64</li> <li>autoinstall/</li> <li>add-passphrase.sh</li> <li>copy-ssh-key.sh</li> <li>hosts.ini</li> <li>ansible/</li> <li>playbook<ul> <li>.kube/</li> <li>j2/</li> </ul> </li> </ul> </li> </ul> <p>```</p>"},{"location":"kubernetes-lab-setup/100-alpine1/101-ssh-to-alpine1/","title":"SSH","text":""},{"location":"kubernetes-lab-setup/100-alpine1/101-ssh-to-alpine1/#steps-to-ssh","title":"Steps to ssh","text":"<p>In order to ssh to <code>alpine1</code> the Hyper V Host will need an IP address from the subnet <code>192.168.100.0/24</code></p> <p>Do not use DHCP to assign the IP address , as the gateway is also assigned to my Hyper-V host Network Interface this will cause network connectivity issues. As there will be 2 gateways configured.</p> <p>This caused some network issues where my connection to the internet kept timing out. When I set the IP address without the Gateway the network issues were no longer present.  So better to set the IP address manually then obtain via DHCP.</p> <p>To make it easier you can use the PowerShell script below.</p> <p>Paste the following PowerShell code int Windows Terminal as Administrator.</p> <pre><code>$ifindex=(Get-NetAdapter | Where-Object { $_.Name -like \"*192.168.100.0*\" }).InterfaceIndex\nNew-NetIPAddress -InterfaceIndex $ifindex -IPAddress 192.168.100.2 -PrefixLength 24\nGet-DnsClient -InterfaceIndex $ifindex | Set-DnsClientServerAddress -ServerAddresses (\"192.168.100.1\")\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/101-ssh-to-alpine1/#validate","title":"Validate","text":"<p>Run this command on Windows Terminal to check if an IP has been assigned</p> <pre><code>get-netipaddress -InterfaceIndex (Get-NetAdapter | Where-Object { $_.Name -like \"*192.168.100.0*\" }).InterfaceIndex\n</code></pre> <p></p> <p>Run a ping test to <code>alpine1</code> , ensure its succesful.</p> <pre><code>ping 192.168.00.1\n</code></pre> <p>Troubleshooting.</p> <ul> <li>If that is working, then review the IP address ranges assigned to ensure they are correct and check for typos in the IP address.</li> <li>Ensure only one gateway IP address is assigned. Having multiple gateways will introduce connectivity issues.</li> </ul>"},{"location":"kubernetes-lab-setup/100-alpine1/101-ssh-to-alpine1/#how-to-ssh-to-alpine1","title":"How to SSH to <code>alpine1</code> ?","text":"<p>Once you can ping <code>alpine1</code> you can now setup a ssh session.</p> <p>Use a terminal client. Putty is a good terminal to use. In Windows 11 as I have Terminal client installed I will be using this lab. But any terminal program can be used.</p> <p>Back to ssh from Windows terminal I will perform further configuration from the Windows Terminal as I will be able to copy and paste commands.</p> <p>From Windows Terminal type the following command. You will be presented with a login request.</p> <pre><code># alpine1 IP address = 192.168.100.1\nssh -l root 192.168.100.1\nPassword : `123` # if the proposed Password was used\n</code></pre> <p></p>"},{"location":"kubernetes-lab-setup/100-alpine1/101-ssh-to-alpine1/#you-are-now-connected","title":"You are now connected!","text":""},{"location":"kubernetes-lab-setup/100-alpine1/102-Clone-repository/","title":"Clone-repository","text":"<p>In this step we will download the files required to setup the various services. We now clone the repository to <code>ansible1</code>.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/102-Clone-repository/#this-script-is-to-download-the-files-from-the-repository-httpsgithubcombelajarpowershellkubernetes-lab","title":"This script is to download the files  from the Repository https://github.com/belajarpowershell/kubernetes-lab","text":"<pre><code># Step 1 clone the repository\n# Step 2 run the script `alpine1-first-run.sh`\n# This script will perform the following\n# - move the `srv` from git folder \"kubernetes-lab/\" to \"/srv\" \n# - enable the community repository to install ansible\n# - install Ansible using run `apk add ansible`\n\n# install git \napk add git\n\n# clone the repository\ngit clone https://github.com/belajarpowershell/kubernetes-lab.git\n\n# change folder\ncd kubernetes-lab\n\n# copy the `srv` from git folder \"kubernetes-lab/\" to \"/srv\" \n# change the script to executable\nchmod +x alpine1-first-run.sh\n\n\n#run the script \n./alpine1-first-run.sh\n\n# files `kubernetes-lab/srv` folder is now copied to /srv\n# this is important as the files required for the setup must be located at `/srv/`\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/102-Clone-repository/#folder-structure","title":"Folder Structure","text":"<ul> <li>/kubernetes-lab<ul> <li>k3s-lab-setup</li> <li>The server sequence of setup</li> </ul> </li> <li>scripts</li> <li>srv/<ul> <li>tftp/</li> <li>bios</li> <li>efi64</li> <li>autoinstall/</li> <li>add-passphrase.sh</li> <li>copy-ssh-key.sh</li> <li>hosts.ini</li> <li>ansible/</li> <li>playbook-ansible1<ul> <li></li> </ul> </li> <li>playbook-k3s<ul> <li>.kube/</li> <li>j2/</li> </ul> </li> </ul> </li> </ul> <p>```</p>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/","title":"Setup services Manual Steps","text":"<p>Setting up the various services for automation is time consuming and possible prone to errors.</p> <p>Using Ansible, I was able to reduce the time it too deploy the services required. </p> <p>Here we use the playbook with the several playbooks as below. </p> <p>In <code>103-setup-using-ansible</code> the steps are combined and implemented in a single command. The individual steps will be helpful for troubleshooting or if there is a need to change selected configuration.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/#ansible-playbook-execution","title":"Ansible Playbook execution","text":"<p>Change directory to <code>/srv/ansible/playbook-alpine1</code></p> <pre><code>cd /srv/ansible/playbook-alpine1\n</code></pre> <p>Execute the playbook run from <code>/srv/ansible/playbook-alpine1</code></p>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/#101-dhcp-server-setup","title":"101 DHCP server Setup","text":"<pre><code>ansible-playbook 101-DHCP-server.yaml\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/#102-setup-alpine1-as-a-router","title":"102 Setup <code>alpine1</code> as a router","text":"<pre><code>ansible-playbook 102-setup-router.yaml\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/#103-dns-setup","title":"103 DNS Setup","text":"<pre><code>ansible-playbook 103-setup-DNS.yaml\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/#104-nginx-setup","title":"104 Nginx Setup","text":"<pre><code>ansible-playbook 104-setup-nginx.yaml\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/#105-nfs-setup","title":"105 NFS  Setup","text":"<pre><code>ansible-playbook 105-nfs.yaml\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/#106-cloud-init-setup","title":"106 Cloud-init Setup","text":"<pre><code>ansible-playbook 106-cloud-init.yaml\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/#108-dhcp-for-pxe-setup","title":"108 DHCP for PXE Setup","text":"<pre><code>ansible-playbook 108-DHCP-for-PXE.yaml\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible-manual/#110-boot-file-setup","title":"110 Boot file setup.","text":"<pre><code>ansible-playbook 110-setup-boot-files-part2-OS.yaml\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible/","title":"Setup services using Ansible","text":"<p>Setting up the various services for automation is time consuming and possible prone to errors.</p> <p>Using Ansible, I was able to reduce the time it too deploy the services required. </p> <p>Here we use the playbook with the complete steps in <code>alpine-services.yaml</code>. In <code>103-setup-using-ansible-manual</code> the individual services are separated. </p>"},{"location":"kubernetes-lab-setup/100-alpine1/103-setup-using-ansible/#ansible-playbook-execution","title":"Ansible Playbook execution","text":"<p>Change directory to <code>/srv/ansible/playbook-alpine1</code></p> <pre><code>cd /srv/ansible/playbook-alpine1\n</code></pre> <p>Execute the playbook run from <code>/srv/ansible/playbook-alpine1</code></p> <pre><code>ansible-playbook alpine-services.yaml\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/113-Install-Ubuntu/","title":"Ubuntu Installation","text":"<p>Step <code>103-setup-using-ansible</code> would have setup <code>alpine1</code> to perform the functions required for the Ubuntu Auto install steps.</p> <p>We now have some manual tasks to create the <code>user-data-mac-address</code> file for each of the Virtual Machines created ( excluding <code>alpine1</code>)</p>"},{"location":"kubernetes-lab-setup/100-alpine1/113-Install-Ubuntu/#step-1","title":"Step 1","text":""},{"location":"kubernetes-lab-setup/100-alpine1/113-Install-Ubuntu/#start-and-stop-all-the-virtual-machines-created","title":"Start and stop all the Virtual Machines created.","text":"<p>For the following step, the Virtual Machines created must be started once to generate the mac-address.</p> <p>From PowerShell ISE  open <code>G:\\kubernetes-lab\\srv\\scripts\\hyper-v_Host\\get-hyperv-start-stopVM.ps1</code></p> <p>Click on the <code>Green Play</code> button, this will execute the script.</p> <p>This will start and then stop all the Virtual Machines ( Load\u204e , master\u204e , worker\u204e  , \u204esingle\u204e  ). Ensure the Virtual Machine names are not changed. </p> <p>You can also Start and Stop from the Hyper-V Console. </p>"},{"location":"kubernetes-lab-setup/100-alpine1/113-Install-Ubuntu/#step-2","title":"Step 2","text":""},{"location":"kubernetes-lab-setup/100-alpine1/113-Install-Ubuntu/#generate-user-data-mac-address-file","title":"Generate user-data-mac-address file.","text":"<p>For a Kubernetes cluster we will need multiple Virtual Machines. This will require multiple <code>user-data-mac-address</code> files to be created. Here I use PowerShell to generate the files.</p> <p>From PowerShell ISE  open <code>G:\\kubernetes-lab\\srv\\scripts\\hyper-v_Host\\get-hyperv-VM-mac.ps1</code></p> <p>Click on the <code>Green Play</code> button, this will execute the script.</p> <p>The generated files are in the subfolder  <code>.\\autoinstall\\</code></p> <p></p> <p>From the configuration in step <code>111-setup-boot-files-part3-pxelinux.cfg</code>  ( refer to manual step 11) , the Boot sequence is looking up the location. <code>http://192.168.100.1/autoinstall/</code></p> <p>This is from the following code</p> <pre><code>APPEND netboot=nfs boot=casper root=/dev/nfs nfsroot=192.168.100.1:/srv/isoubuntu autoinstall \n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/113-Install-Ubuntu/#copy-autoinstall-files-to-alpine1","title":"Copy autoinstall files to <code>alpine1</code>","text":"<p>Using WinSCP copy the files created to the folder  <code>/srv/autoinstall/</code> on the <code>alpine1</code> server. The duplicate file with the node name is workaround to show the hostname of the specific file. </p> <p></p> <p>From step <code>104-setup-nginx</code>( refer to manual steps), the folder /srv/ was exposed to be visible from <code>http://192.168.100.1/</code>. All folders created in <code>/srv/</code> will be listed via the browser.</p> <p>Validate by browsing <code>http://192.168.100.1/</code></p> <p></p> <p>// To update Video of Script generation and the Ubuntu setup.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/113-Install-Ubuntu/#lessons-learnt","title":"Lessons Learnt","text":"<ul> <li>The MAC addresses for a newly created Hyper-V Virtual Machine is not generated until its started at least once. </li> <li>the <code>meta-data</code> and<code>vendor-data</code> while empty must exist.</li> </ul>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/101-DHCP-server/","title":"Install and Configure DHCP","text":"<p>For the initial setup we will have the basic DHCP configuration.</p> <pre><code>#install dhcpd\napk add dhcp\n</code></pre> <p>Update the file /etc/dhcp/dhcpd.conf with the content below.</p> <p>Use VI editor to edit ( or any other you are familiar with) <pre><code># using console you cannot copy paste, must type in manually\n# create config file \ntouch /etc/dhcp/dhcpd.conf\n# edit config file\nvi /etc/dhcp/dhcpd.conf\n\nsubnet 192.168.100.0 netmask 255.255.255.0 {\n  range 192.168.100.100 192.168.100.200;\n  option domain-name-servers 192.168.100.1;\n  option routers 192.168.100.1;\n}\n[esc] :x[enter] to save and exit\n</code></pre></p> <p>routers : The routers IP usually is the WIFI or WAN router IP. In this lab setup we want emulate an office network setup. In the subsequent steps we will review how to setup alpine1 as a Router.</p> <p>domain-name-servers : in this setup we want to resolve some internal hostnames  that cannot be resolved by public DNS servers. We want to have alpine1 to function as DNS server and also forward DNS request not resolved locally. The steps to configure alpine1 as DNS server is reviewed later.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/101-DHCP-server/#restart-the-dhcp-deamon","title":"Restart the DHCP deamon","text":"<p>To refresh the updated configuration restart the DHCP services. Any errors in configuration will appear at this point. <pre><code># set dhcpd to start at boot\nrc-update add dhcpd default\n\n# start the dhcpd service.\nrc-service dhcpd start\n\n# Check the dhcpd service status.\nrc-service dhcpd status\n</code></pre> DHCP logs Later you may need to check if the IP addresses are assigned etc. These to files will be key for your troubleshooting. <pre><code>cat /etc/dhcp/dhcpd.conf\ncat /var/lib/dhcp/dhcpd.leases\n</code></pre></p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/101-DHCP-server/#dhcp-setup-is-complete-for-alpine1-server","title":"DHCP Setup is complete  for alpine1 server .","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/101-DHCP-server/#steps-to-ssh","title":"Steps to ssh.","text":"<p>If using DHCP, the gateway is also assigned to my Hyper-V host Network Interface. This caused some network issues where my connection to the internet kept timing out. When I set the IP address without the Gateway the network issues were no longer present.  So better to set the IP address manualy then obtain via DHCP.</p> <p>Paste the following Powershell code int Windows Terminal as Administrator.</p> <p><pre><code>$ifindex=(Get-NetAdapter | Where-Object { $_.Name -like \"*192.168.100.0*\" }).InterfaceIndex\nNew-NetIPAddress -InterfaceIndex $ifindex -IPAddress 192.168.100.2 -PrefixLength 24\nGet-DnsClient -InterfaceIndex $ifindex | Set-DnsClientServerAddress -ServerAddresses (\"192.168.100.1\")\n</code></pre> Validate</p> <p>Run this command on Windows Terminal to check if an IP has been assigned <pre><code>get-netipaddress -InterfaceIndex (Get-NetAdapter | Where-Object { $_.Name -like \"*192.168.100.0*\" }).InterfaceIndex\n</code></pre> </p> <p>Troubleshooting. - If that is working, then review the IP address ranges assigned to ensure they are correct and check for typos in the IP address. - Ensure only one gateway IP address is assigned. Having multiple gateways will introduce connectivity issues.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/101-DHCP-server/#how-to-identify-the-ip-address-on-a-linux-machine","title":"How to identify the IP address on a linux machine?","text":"<p>At the prompt type. <pre><code>  ip a \n</code></pre> Here is an example  </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/101-DHCP-server/#how-to-ssh-to-a-linux-machine","title":"How to SSH to a Linux Machine?","text":"<p>Use a terminal client.Putty is a good terminal to use. In Windows 11 as I have Terminal client installed I will be using this lab. But any terminal program can be used.</p> <p>Back to ssh from Windows terminal I will peform further configuration from the Windows Terminal as I will be able to copy and paste commands.</p> <p>From Windows Terminal type the following command. You will be presented with a login request.</p> <p><pre><code>ssh -l root 192.168.100.1\nPassword : `123` # if the proposed Password was used\n</code></pre> </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/101-DHCP-server/#dhcp-is-installed-and-ssh-to-alpine1-server-established","title":"DHCP is installed and SSH to <code>alpine1</code> server established.","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/101-DHCP-server/#next-step","title":"Next step","text":"<p>We will proceed with the router installation. </p> <p>The validation for DHCP and Internet on the private network will be tested at a later stage.</p> <p>Please continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/101-DHCP-server/#102-alpine1-setup-route","title":"102-alpine1-setup-route","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/102-setup-router/","title":"Configure as a Router","text":"<p>In this setup, <code>alpine1</code> will function as an internet router for the servers in the  <code>Private 192.168.100.0/24</code> network.</p> <p>This function is to allow the devices in the <code>Private 192.168.100.0/24</code> network to access the internet via <code>alpine1</code>. </p> <p>The configuration is simple.</p> <p>ssh to <code>alpine1</code> and configure as below. <pre><code># paste the following in the alpine1 ssh terminal\n# if you are on the Hype-V console you cannot paste the text\necho \"net.ipv4.ip_forward=1\" |  tee -a /etc/sysctl.conf\n\n# reload the config using\nsysctl -p\n\n# enable IP routing on alpine1\napk add iptables\n\nrc-update add iptables\n\niptables -A FORWARD -i eth1 -j ACCEPT\n\n# eth0 is the external interface (connected to the internet)\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE \n/etc/init.d/iptables save\n</code></pre></p> <p>Reference link to setup Alpine as a router</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/102-setup-router/#setup-alpine1-server-router-is-completed","title":"Setup alpine1 server router is completed .","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/102-setup-router/#next-step","title":"Next step","text":"<p>We will proceed with the DNS server installation </p> <p>Please continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/102-setup-router/#103-alpine1-setup-dns","title":"103-alpine1-setup-dns","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/103-setup-dns/","title":"Configure DNS server","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/103-setup-dns/#install-bind-dns-server-and-tools-dig-and-nslookup-on-alpine1","title":"Install bind ( DNS server) and tools dig and nslookup on alpine1.","text":"<p>DNS resolves Names to IP addresses. Kubetnetes will need the DNS to be closer to a production setup. Hence the requirement. This service can be skipped , but then all the connectivity will be via IP alone.</p> <p>In this lab we will setup <code>bind</code> as the DNS server application.</p> <pre><code>#install bind\napk add bind bind-tools\n\n# set named to start at boot\nrc-update add named default\n\n# check if the tools are installed.\ndig -v\nnslookup -v\n</code></pre> <p>Configure bind with a zone and resolve internet names from the configured forwarders. <pre><code># create named.conf file.\ntouch /etc/bind/named.conf\n\n#edit named.conf file \nvi /etc/bind/named.conf\n\n# replace or edit the file to reflect the following.\n\noptions {\n     listen-on port 53 { 127.0.0.1; 192.168.100.1; };\n     forwarders { 8.8.8.8; 8.8.4.4; };\n     directory \"/var/bind\";\n     dump-file \"/var/bind/data/cache_dump.db\";\n     statistics-file \"/var/bind/data/named_stats.txt\";\n     memstatistics-file \"/var/bind/data/named_mem_stats.txt\";\n     allow-query { localhost; 192.168.100.0/24; };\n     recursion yes;\n};\nzone \"k8s.lab\" IN {\n        type master;\n        file \"/etc/bind/master/k8s.lab\";\n};\n</code></pre> Next, we create the zone file. In this lab the zone name  is <code>k8s.lab</code></p> <pre><code># create folder and file\nmkdir - p /etc/bind/master/ &amp;&amp; touch /etc/bind/master/k8s.lab\n\n# edit the file \nvi /etc/bind/master/k8s.lab\n# paste following \n$TTL 38400\n@ IN SOA ns.k8s.lab admin.k8s.lab. (\n2       ;Serial\n600     ;Refresh\n300     ;Retry\n60480   ;Expire\n600 )   ;Negative Cache TTL\n\n@       IN      NS      ns1.k8s.lab.\nns1     IN      A       192.168.100.1\nalpine1         IN      A       192.168.100.1\nk8s-ha-cluster  IN      A       192.168.100.201\nloadbalancer    IN      A       192.168.100.201\nmaster1         IN      A       192.168.100.202\nmaster2         IN      A       192.168.100.203\nmaster3         IN      A       192.168.100.204\nworker1         IN      A       192.168.100.205\nworker2         IN      A       192.168.100.206\nworker3         IN      A       192.168.100.207\nxsinglenode     IN      A       192.168.100.199\n</code></pre> <p>Validate the bind configuration <pre><code># check if the formating is correct \nnamed-checkconf /etc/bind/named.conf\n\n# (re)start bind service. ( the service name is 'named') \nrc-service named restart \n\n# ensure no errors are returned\n</code></pre></p> <p>Validate DNS.</p> <p><pre><code>To validate you need to run nslookup on a remote server. \nAs there are no other servers setup, this can be validated when the VM's are created.\n</code></pre> To ensure <code>alpine1</code> resolves the FQDN's correctly ensure the following is configured </p> <pre><code># on `alpine1`  run command to check Network interface configuration\nvi /etc/network/interfaces\n\nauto lo\niface lo inet loopback\n\nauto eth0\niface eth0 inet dhcp\n\nauto eth1\niface eth1 inet static\n        address 192.168.100.1\n        netmask 255.255.255.0\n## ensure the following 2 rows are added.\ndns-search k8s.lab  \ndns-nameservers 192.168.100.1 \n</code></pre> <pre><code>#on alpine1 only.\nvi /etc/resolve\n\nnameserver 192.168.100.1\nsearch k8s.lab\n\n#in my case the every time the alpine1 is rebooted the values in this file get reset.\n# as such the following is to set the immutable flag \n\n#enables readonly\nchattr +i /etc/resolv.conf \n\n#disables readonly\nchattr -i /etc/resolv.conf \n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/103-setup-dns/#setup-alpine1-server-dns-is-complete","title":"Setup <code>alpine1</code> server DNS is complete.","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/103-setup-dns/#next-step","title":"Next step","text":"<p>We will proceed with the nginx server installation </p> <p>Please continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/103-setup-dns/#104-alpine1-setup-nginx","title":"104-alpine1-setup-nginx","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/104-setup-nginx/","title":"Install and Configure Nginx","text":"<p>A webserver is handy for several reasons. In this Lab we will use a webserver for quite a bit of stuff. Lets set up <code>nginx</code> for this purpose.</p> <pre><code>#install nginx \napk update\napk add nginx\n\n# set nginx to start at boot\nrc-update add nginx default\n\n#check nginx service status\nrc-service nginx status\n\n# start nginx service \nrc-service nginx start\n\n\n#nginx configuration\n# no change required in this lab setup\nvi /etc/nginx/nginx.conf\n</code></pre> <p>Configure a new site to list the files over http. <pre><code># nginx installs with a default site \n# this site will intefere with the new site we are setting up.\n# remove default site\nrm /etc/nginx/http.d/default.conf\n\n# create a file for the site .\n\nvi /etc/nginx/http.d/installfileserver.conf\n# paste the following code.\nserver {\n    server_name localhost;\n    root /srv/;\n    #index index.html;\n    location / {  # new url path # this is the URL path on browser\n  alias /srv/; # directory to list\n    #in this case http://ip/ will list files in \"/srv/\"\n    # this might not be best practice, but this is a Lab setup \n    autoindex on;\n    }\n\n}\n</code></pre></p> <p>Check nginx config after every change for errors <pre><code>#check syntax \nnginx -t\n\n#reload nginx\nrc-service nginx restart \n</code></pre> Logs You can check the files accessed via url, this way you can validate if the correct paths are in place.</p> <pre><code>cat /var/log/nginx/access.log\n</code></pre> <p>Validate </p> <p><pre><code># create a file to be listed\nmkdir -p /srv/autoinstall/ &amp;&amp; touch /srv/autoinstall/test.txt\n\n#In a browser open the following URL\nhttp://192.168.100.1\n\nThe list of files hosted in `/srv/autoinstall/` will be listed.\n</code></pre> Here is and example </p> <p><code>nginx</code> can also be deployed using <code>Docker</code>, I will provide the steps at a later version</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/104-setup-nginx/#setup-alpine1-server-nginx-is-complete","title":"Setup <code>alpine1</code> server <code>nginx</code> is complete.","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/104-setup-nginx/#next-step","title":"Next step","text":"<p>We will proceed with the nfs server installation </p> <p>Please continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/104-setup-nginx/#105-alpine1-nfs","title":"105-alpine1-nfs","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/105-nfs/","title":"Install and Configure <code>nfs</code>","text":"<p><code>nfs</code> functions as a remote Hard Disk. If you need to access files locally but are stored on a remote server shared using <code>nfs</code> then this <code>nfs</code> will behave as its a Hard Disk setup locally.</p> <p>We do have use cases for <code>nfs</code> so lets set this up. </p> <pre><code># in this lab we will mount the ubuntu ISO to be served via nfs.\n# the first part will be to mount the ubuntu ISO files to a folder.\n# Copy and paste each row without a # at the begining.\n\n\n# create the folder to download the iso file to.\nmkdir -p /srv/tftp/iso\n# Download the ISO\nwget -P /srv/tftp/iso https://releases.ubuntu.com/20.04.6/ubuntu-20.04.6-live-server-amd64.iso\n#Create folder to mount ISO file\nmkdir -p /srv/isoubuntu\n# this will mount the ISO file until the server is rebooted\nmount -o loop,ro -t iso9660 /srv/tftp/iso/ubuntu-20.04.6-live-server-amd64.iso /srv/isoubuntu\nls /srv/isoubuntu\n\n# to make the iso mounted permanantly\n# update the fstab\nc\n#add following entry\n/srv/tftp/iso/ubuntu-20.04.6-live-server-amd64.iso /srv/isoubuntu iso9660 loop 0 0\n\nValidate by rebooting the server and check if the iso files are listed.\nls /srv/isoubuntu\n\n\n#install nfs \napk update\napk add nfs-utils\n\n# set nfs to start at boot\nrc-update add nfs\n\n# check status\nrc-service nfs status\n\n# start nfs service\nrc-service rpcbind start\nrc-service nfs start\n\n\n#### revalidate config and validation\n\n#nfs configuration \n# update file with the entry below.\nvi /etc/exports\n\n/srv/isoubuntu 192.168.100.1/24(async,ro,no_subtree_check,no_root_squash)\n# refresh the nfs with the updated row.\n\nexportfs -arv\n\n#test mount using the following command\nmkdir mntnfs\nmount 192.168.100.1:/srv/isoubuntu mntnfs\n\n# List files in mount \nls mntnfs/\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/105-nfs/#setup-alpine1-nfs-is-complete","title":"Setup <code>alpine1</code> <code>nfs</code> is complete.","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/105-nfs/#next-step","title":"Next step","text":"<p>We will proceed with the tftp server installation </p> <p>Please continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/105-nfs/#106-alpine1-tftp","title":"106-alpine1-tftp","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/106-tftp/","title":"Install and Configure tftpd","text":"<p>tftp is required for PXELINUX boot setup. This is how the initial boot files are retrieved from the remote client.</p> <pre><code># create directories\nmkdir /srv/tftp/\n\n#install tftpd \napk update\napk add tftp-hpa\n\n# set nginx to start at boot\nrc-update add in.tftpd\n\n# Update in.tftpd configuration\nvi /etc/conf.d/in.tftpd\n  #change or update row to reflect the path /srv/tftp/\n  INTFTPD_PATH=\"/srv/tftp/\"\n\n#check in.tftpd  service status\nrc-service in.tftpd status\n\n#Start service now\nrc-service in.tftpd start\n</code></pre> <p>The path <code>/srv/tftp/</code> is important to note as the subsequent configuration will use this folder as root.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/106-tftp/#next-step","title":"Next step","text":"<p>We will proceed with the cloud-init installation </p> <p>Please continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/106-tftp/#107-alpine1-cloud-init","title":"107-alpine1-cloud-init","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/107-cloud-init/","title":"Install and Configure cloud-init","text":"<p>cloud-init is the application that reads the <code>user-data</code> script as part of autoinstallation.  As I have been having issues with my <code>user-data</code> I needed a method to validate the content. Turns out <code>cloud-init</code> has a command for just that!</p> <pre><code># enable community repository\nvi /etc/apk/repositories\nremove # from the row\n#http://mirror.jingk.ai/alpine/v3.18/community\n\n#install `cloud-init` \napk update\napk add cloud-init\n\n#setup cloud-init\nsetup-cloud-init\n</code></pre> <p>Validate <code>user-data</code> using the command below. At this time the lab has not generated the <code>user-data</code>  file . </p> <p>You can test this if you have a <code>user-data</code> file handy. The <code>user-data</code> will be shared in later steps </p> <p><pre><code>cloud-init schema --config-file user-data\n</code></pre> My error was to put in an entry for remark that was not recognized.  </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/107-cloud-init/#next-step","title":"Next step","text":"<p>We will proceed with the DHCP setup for PXE requirements. </p> <p>Please continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/107-cloud-init/#108-alpine1-dhcp-for-pxe","title":"108-alpine1-DHCP-for-PXE","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/108-DHCP-for-PXE/","title":"DHCP options for PXE boot","text":"<p>The initial DHCP was to provide IP addresses to remote clients.  As we now want to provide the scope options for the tftp servers with the boot file location, lets make the following changes.</p> <p>Update the dhcpd.conf as follows. <pre><code>vi /etc/dhcp/dhcpd.conf\n\nsubnet 192.168.100.0 netmask 255.255.255.0 {\n   range 192.168.100.50 192.168.100.100;\n   option routers 192.168.100.1;\n   option domain-name-servers 192.168.100.1;\n   option time-servers 192.168.100.1;\n   option ntp-servers 192.168.100.1;\n   # next-server 192.168.100.1; # this is typically if the tftp is on another server.\n   if substring(option vendor-class-identifier, 0, 20) = \"PXEClient:Arch:00000\" {\n         filename \"bios/pxelinux.0\";\n   }\n   if substring(option vendor-class-identifier, 0, 20) = \"PXEClient:Arch:00007\" {\n         filename \"efi64/syslinux.efi\";\n   }\n   #assign IP based on MAC address\n   # if you  have a need to setup DHCP for specific hosts \n   host someserver {\n        hardware ethernet 00:15:5d:00:8a:3d; # change to your MAC address. \n        fixed-address 192.168.100.250;\n        option host-name \"someserver\";\n    }\n}\n</code></pre> Restart the dhcp service. <pre><code>rc-service dhcpd restart\n</code></pre> Validation</p> <p>At this stage we cannot perform test to validate if this works. This is because we have not setup the files for the boot process. Lets defer the test after the boot files have been setup.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/108-DHCP-for-PXE/#next-step","title":"Next step","text":"<p>We will proceed with the DHCP setup for PXE requirements. </p> <p>Please continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/108-DHCP-for-PXE/#109-setup-boot-files-part1-pxe","title":"109-setup-boot-files-part1-PXE","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/109-setup-boot-files-part1-PXE/","title":"Setup boot files for PXELINUX","text":"<p>The boot files are separated by the type of Boot process <code>BIOS</code> and <code>EFI</code> Take note the files while have duplicate names, there are separate versions for <code>BIOS</code> and <code>EFI</code>.</p> <p>If you had followed the steps from 000-ReadMeFirst, you would already have the following files available.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/109-setup-boot-files-part1-PXE/#tftp-directory-and-content","title":"TFTP Directory and content","text":"<ul> <li>(/srv/tftp/bios) <code>pxelinux.0</code> <code>ldlinux.c32</code> <code>menu.c32</code> <code>libutil.c32</code> <code>vesamenu.c32</code> <code>libcom32.c32</code> <code>pxelinux.cfg\\default</code></li> <li>(/srv/tftp/efi64) <code>ldlinux.e64</code> <code>syslinux.efi</code> <code>menu.c32</code> <code>libutil.c32</code> <code>vesamenu.c32</code> <code>libcom32.c32</code> <code>pxelinux.cfg\\default</code></li> </ul>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/109-setup-boot-files-part1-PXE/#next-step","title":"Next step","text":"<p>We will proceed with the preparation of the OS boot files. Continue with </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/109-setup-boot-files-part1-PXE/#110-setup-boot-files-part2-os","title":"110-setup-boot-files-part2-OS","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/110-setup-boot-files-part2-OS/","title":"Setup Ubuntu OS Boot files","text":"<p>In this section we will perform 2 tasks  - Download and mount the ISO Live CD. - Extract the <code>vmlinuz</code> and <code>initrd</code> files from the LiveCD to the tftp folder</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/110-setup-boot-files-part2-OS/#step-1-download-and-mount-the-iso","title":"Step 1 Download and Mount the ISO.","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/110-setup-boot-files-part2-OS/#_1","title":"Setup Ubuntu OS Boot files","text":"<pre><code># lets host the ISO in the folder /iso\nmkdir -p /srv/tftp/iso\n# Download the ISO\n[ ! -f /srv/tftp/iso/ubuntu-20.04.6-live-server-amd64.iso ] &amp;&amp; wget -P /srv/tftp/iso https://releases.ubuntu.com/20.04.6/ubuntu-20.04.6-live-server-amd64.iso\n\n# Create mount folder \nmkdir -p /srv/isoubuntu\n\n# mount the ISO to view contents.\nmount -o loop,ro -t iso9660 /srv/tftp/iso/ubuntu-20.04.6-live-server-amd64.iso /srv/isoubuntu\n# list content to validate\nls /srv/isoubuntu\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/110-setup-boot-files-part2-OS/#step-2-extract-vmlinuz-and-initrd","title":"Step 2 Extract <code>vmlinuz</code> and <code>initrd</code>","text":"<p>Run the commands below to extract the boot files.</p> <pre><code>#create ubuntu folder to store the boot files.\n# this way we can have boot files for multple distros if required.\nmkdir -p /srv/tftp/ubuntu/casper\n\n# copy files\ncp  /srv/isoubuntu/casper/vmlinuz /srv/tftp/ubuntu/casper\ncp  /srv/isoubuntu/casper/initrd /srv/tftp/ubuntu/casper\n</code></pre> <p>Validate The boot files will be served via http. Do validate if this folder and content is listed.</p> <p></p> <p>Validate if accessible via HTTP </p> <p>As we have already setup <code>nginx</code> to list directory content. This should be visible via browser.</p> <p> Click <code>tftp</code> and <code>iso</code> to list the ISO LiveCD.</p> <p></p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/110-setup-boot-files-part2-OS/#step-2-extract-vmlinuz-and-initrd_1","title":"Step 2 Extract <code>vmlinuz</code> and <code>initrd</code>","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/110-setup-boot-files-part2-OS/#boot-files","title":"Boot files.","text":"<p>When booting a Server with Linux, there are 2 files required. <code>vmlinuz</code> and <code>initrd</code> at times the version is part of the name depending on Linux flavors</p> <p>In this example boot files are retrieved from <code>ubuntu-20.04.6-live-server-amd64.iso</code></p> <p>Run the commands below to extract the boot files.</p> <pre><code>#create ubuntu folder to store the boot files.\n# this way we can have boot files for multple distros if required.\nmkdir -p /srv/tftp/ubuntu/casper\n\n# copy files\ncp  /srv/isoubuntu/casper/vmlinuz /srv/tftp/ubuntu/casper\ncp  /srv/isoubuntu/casper/initrd /srv/tftp/ubuntu/casper\n</code></pre> <p>Validate The boot files will be served via http. Do validate if this folder and content is listed.</p> <p></p> <p>## Next step</p> <p>At this stage we have made all the required files available for the boot process. Next we will put in the final piece for the setup to function.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/110-setup-boot-files-part2-OS/#111-setup-boot-files-part3-pxelinuxcfg","title":"111-setup-boot-files-part3-pxelinux.cfg","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/111-setup-boot-files-part3-pxelinux.cfg/","title":"Configure PXE Boot menu","text":"<p>Configure the boot menu <code>pxelinux.cfg/default</code> The final piece is to prepare the boot menu to load and present a selection that will install the Ubuntu OS.</p> <p>This file is already with the configuration below.</p> <p>Validate the file content of <code>pxelinux.cfg/default</code> matches the code below. <pre><code>vi /srv/tftp/efi64/pxelinux.cfg/default\n\nDEFAULT Bootlocal\nTIMEOUT 50\nUI vesamenu.c32\nMENU RESOLUTION 1024 768\nMENU BACKGROUND splash/pine-green-splash.png\nMENU TITLE PXE Boot Menu\n\n\nLABEL ubuntu-iso-remote #iso downloaded from http. Requires 4GB of Memory\n    MENU LABEL ubuntu-iso-remote (iso downloaded - 4GB Memory)\n    KERNEL http://192.168.100.1/tftp/ubuntu/casper/vmlinuz\n    INITRD http://192.168.100.1/tftp/ubuntu/casper/initrd\n    append url=http://192.168.100.1/tftp/iso/ubuntu-20.04.6-live-server-amd64.iso autoinstall ds=nocloud-net;s=http://192.168.100.1/autoinstall/ ip=dhcp fsck.mode=skip ---\n\nLABEL ubuntu-DVD-local (Ubuntu  DVD locally mounted - 1GB Memory)\n    MENU LABEL ubuntu-iso-local (iso locally mounted 1GB of Memory)\n    KERNEL http://192.168.100.1/tftp/ubuntu/casper/vmlinuz\n    INITRD http://192.168.100.1/tftp/ubuntu/casper/initrd\n    APPEND autoinstall ds=nocloud-net;s=http://192.168.100.1/autoinstall/ ip=dhcp fsck.mode=skip ---\n\nLABEL ubuntu-nfs-boot (Ubuntu  iso mounted on nfs - 1GB Memory)\n    MENU LABEL ubuntu-iso-local (iso locally mounted 1GB of Memory)\n    KERNEL http://192.168.100.1/tftp/ubuntu/casper/vmlinuz\n    INITRD http://192.168.100.1/tftp/ubuntu/casper/initrd\n    APPEND netboot=nfs boot=casper root=/dev/nfs nfsroot=192.168.100.1:/srv/isoubuntu autoinstall ds=nocloud-net;s=http://192.168.100.1/autoinstall/ ip=dhcp fsck.mode=skip ---\n\nLABEL Bootlocal\n    MENU LABEL Bootlocal \n    LOCALBOOT 0\n</code></pre> With the above configuration in place, a remote server in the <code>Private 192.168.100.0/24</code> network that has Network boot configured will be able to boot successfully. </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/111-setup-boot-files-part3-pxelinux.cfg/#what-happens-next","title":"What happens next?","text":"<p>When the remote client boots, it will get the DHCP options for the <code>tftp</code> server. The <code>syslinux.efi</code> will load and then present the boot menu. Choosing either option will present with the Ubuntu install window. The usual process to setup can now continue.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/111-setup-boot-files-part3-pxelinux.cfg/#lessons-learnt","title":"Lessons Learnt","text":"<p>As I performed the configuration, I faced some issues , and the following had configuration errors mostly. i.e. wrong IP, typos etc.  - Check DHCP   Check the logs to see if the IP is being assigned. If it does not it could be a network related configuration.  - Check nginx  If a URL is part of the process, check the <code>nginx</code> access logs for reference to queries to the specific url.  - ensure VM has minimum 4GB of memory to load the Ubuntu OS setup. This is for the ISO to be extracted to memory. - URL path incorrect. - the selection of <code>ubuntu-iso-local`` is actualy looking at for the ISO to be mounted locally. The files are not loaded from</code>nfs`. </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/111-setup-boot-files-part3-pxelinux.cfg/#next-step","title":"Next step","text":"<p>The PXE boot to install Ubuntu is now complete.  If you are looking to automate the entries for the Ubuntu installation, the next step will review the relevant points.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/111-setup-boot-files-part3-pxelinux.cfg/#113-new-vm-setup-ubuntu-autoinstall","title":"113-New-VM-setup-Ubuntu-autoinstall","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/112-New-single-VM-setup-Ubuntu-autoinstall/","title":"Ubuntu Autoinstall on single VM","text":"<p>One other aspect I was looking at is to automate all the servers to access the configuration files that is reference with the hostname,IP,gateway and Disk config. This way every time I have to rebuild the lab, it will be quicker. </p> <p>I understand from the Ubuntu Automated Server installation that there is a change from version 20.04 a new method is implemented. As such most of the information gathered are from the Ubuntu forums.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/112-New-single-VM-setup-Ubuntu-autoinstall/#these-are-some-references-i-used-to-prepare-the-autoinstall-configuration","title":"These are some references I used to prepare the autoinstall configuration","text":"<p>autoinstall-quickstart</p> <p>fetch-autoinstall-based-on-mac</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/112-New-single-VM-setup-Ubuntu-autoinstall/#lets-get-to-the-details","title":"Lets get to the details","text":"<p>The autoinstall  requires <code>user-data</code> , <code>vendor-data</code> and <code>meta-data</code>. The <code>meta-data</code> and , <code>vendor-data</code>  are blank files.  <code>user-data</code> however must be populated with some valid content. In the file <code>/srv/tftp/efi64/pxelinux.cfg/default</code>, the append row below looks for the <code>user-data</code> and <code>meta-data</code> we need to ensure these file exists is referenced here.</p> <p><pre><code>`autoinstall ds=nocloud-net;s=http://192.168.100.1/autoinstalldata/`\n</code></pre> Example of <code>user-data</code>. This was obtained from a manual installation of <code>ubuntu-20.04.6-live-server-amd64.iso</code>. The file <code>autoinstall-user-data</code> is found in the folder <code>/var/log/installer/autoinstall-user-data</code>. It is best to generate one to store the relevant configurations and then customize based on individual server requirements.</p> <p>Lets create this file.</p> <pre><code># this is a blank file\ntouch /srv/autoinstall/meta-data\ntouch /srv/autoinstall/vendor-data\ntouch /srv/autoinstall/user-data\n\nvi /srv/autoinstall/user-data\n\n#cloud-config\nautoinstall:\n  apt:\n    disable_components: []\n    geoip: true\n    preserve_sources_list: false\n    primary:\n    - arches:\n      - amd64\n      - i386\n      uri: http://my.archive.ubuntu.com/ubuntu\n    - arches:\n      - default\n      uri: http://ports.ubuntu.com/ubuntu-ports\n  drivers:\n    install: false\n  identity:\n    hostname: master1\n    password: $5$1hs/5SKiGm.zNbWk$Ap5W6Tc.YCaALtSp1INrLmYD/GIpRemhpRwtZIZCSO9\n    realname: ss\n    username: ubuntu\n  kernel:\n    package: linux-generic\n  keyboard:\n    layout: us\n    toggle: null\n    variant: ''\n  locale: en_US.UTF-8\n  network:\n    ethernets:\n      eth0:\n        addresses:\n        - 192.168.100.202/24\n        gateway4: 192.168.100.1\n        nameservers:\n          addresses:\n          - 192.168.100.1\n          search: []\n    version: 2\n  ssh:\n    allow-pw: true\n    authorized-keys: []\n    install-server: true\n  storage:\n    config:\n    - ptable: gpt\n      path: /dev/sda\n      wipe: superblock\n      preserve: false\n      name: ''\n      grub_device: false\n      type: disk\n      id: disk-sda\n    - device: disk-sda\n      size: 1127219200\n      wipe: superblock\n      flag: boot\n      number: 1\n      preserve: false\n      grub_device: true\n      type: partition\n      id: partition-0\n    - fstype: fat32\n      volume: partition-0\n      preserve: false\n      type: format\n      id: format-0\n    - device: disk-sda\n      size: 2147483648\n      wipe: superblock\n      flag: ''\n      number: 2\n      preserve: false\n      grub_device: false\n      type: partition\n      id: partition-1\n    - fstype: ext4\n      volume: partition-1\n      preserve: false\n      type: format\n      id: format-1\n    - device: disk-sda\n      size: 50410291200\n      wipe: superblock\n      flag: ''\n      number: 3\n      preserve: false\n      grub_device: false\n      type: partition\n      id: partition-2\n    - name: ubuntu-vg\n      devices:\n      - partition-2\n      preserve: false\n      type: lvm_volgroup\n      id: lvm_volgroup-0\n    - name: ubuntu-lv\n      volgroup: lvm_volgroup-0\n      size: 25203572736B\n      wipe: superblock\n      preserve: false\n      type: lvm_partition\n      id: lvm_partition-0\n    - fstype: ext4\n      volume: lvm_partition-0\n      preserve: false\n      type: format\n      id: format-2\n    - path: /\n      device: format-2\n      type: mount\n      id: mount-2\n    - path: /boot\n      device: format-1\n      type: mount\n      id: mount-1\n    - path: /boot/efi\n      device: format-0\n      type: mount\n      id: mount-0\n  updates: security\n  version: 1\n</code></pre> <p>Generate the crypted password using the command below. The result will change with every run as there is salting present in the code.You don't have to do this if you are extracting the <code>user-data</code> from an existing installation.</p> <pre><code>mkpasswd --method=sha-512 123 # 123 is the password in this example\n</code></pre>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/112-New-single-VM-setup-Ubuntu-autoinstall/#validate","title":"Validate","text":"<p>Validate <code>user-data</code> using the command below.</p> <pre><code>cloud-init schema --config-file user-data\n</code></pre> <p>We now have all the files in place for a machine to start up and autoinstall ubuntu.</p> <p>With the setup in place you can now start the Virtual Machine <code>master1</code>, the network boot will present the Menu, select the option <code>ubuntu-iso-local</code> the VM will boot up with and the installation will complete without manual input.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/112-New-single-VM-setup-Ubuntu-autoinstall/#video-overview-on-the-ubuntu-autoinstall-process","title":"Video overview on the Ubuntu Autoinstall process .","text":"<p>//</p> <p>This a video covering the initial boot, Ubuntu installation and the booting to the newly installed Ubuntu. Autoinstall-Ubuntu-master1</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/112-New-single-VM-setup-Ubuntu-autoinstall/#lessons-learnt","title":"Lessons Learnt.","text":"<p>As I performed the configuration, I faced some issues , and the following had configuration errors mostly. i.e. wrong IP, typos etc.</p> <ul> <li> <p>check dhcp  Ensure the IP Addresses are assigned correctly.</p> </li> <li> <p>Check nginx  You can use the access log to check if a URL has been accessed. This way you can guess where the errors could be.</p> </li> <li> <p>Ensure Virtual Machine specifications meet the minimum requirements.</p> </li> </ul> <p>I had set the Memory to 512MB and enabled Dynamic memory of 512MB to 2048GB but this caused the autoinstall to crash and not complete.</p> <ul> <li> <p>if you choose <code>ubuntu-iso-remote</code> ensure VM has minimum 4GB of memory to load the Ubuntu OS setup. This is for the ISO to be extracted to memory.</p> </li> <li> <p>URL path incorrect   Many times I found myself making simple mistakes in the path. Double check if you did the same too.</p> </li> <li> <p><code>meta-data</code> and <code>vendor-data</code> not created or path configured does not have <code>meta-data</code> and <code>vendor-data</code> </p> </li> </ul>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/113-generate-user-data-multipleVM/","title":"Install Ubuntu","text":"<p>Step <code>103-setup-using-ansible</code> would have setup <code>alpine1</code> to perform the functions required for the Ubuntu Auto install steps.</p> <p>We now have some manual tasks to create the <code>user-data-mac-address</code> file for each of the Virtual Machines created ( excluding <code>alpine1</code>)</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/113-generate-user-data-multipleVM/#step-1","title":"Step 1","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/113-generate-user-data-multipleVM/#start-and-stop-all-the-virtual-machines-created","title":"Start and stop all the Virtual Machines created.","text":"<p>For the following step, the Virtual Machines created must be started once to generate the mac-address.</p> <p>From PowerShell ISE  open <code>G:\\kubernetes-lab\\srv\\scripts\\get-hyperv-startVM.ps1</code></p> <p>Click on the <code>Green Play</code> button, this will execute the script.</p> <p>This will start and then stop all the Virtual Machines . Ensure the Virtual Machine names are not changed. </p> <p>You can also Start and Stop from the Hyper-V Console. </p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/113-generate-user-data-multipleVM/#step-2","title":"Step 2","text":""},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/113-generate-user-data-multipleVM/#generate-user-data-mac-address-file","title":"Generate user-data-mac-address file.","text":"<p>For a Kubernetes cluster we will need multiple Virtual Machines. This will require multiple <code>user-data-mac-address</code> files to be created. Here I use PowerShell to generate the files.</p> <p>From PowerShell ISE  open <code>G:\\kubernetes-lab\\srv\\scripts\\get-hyperv-VM-mac.ps1</code></p> <p>Click on the <code>Green Play</code> button, this will execute the script.</p> <p>The generated files are in the subfolder  <code>.\\autoinstall\\</code></p> <p></p> <p>From the configuration in step 112 , the Boot sequence is looking up the location. <code>http://192.168.100.1/autoinstall/</code></p> <p>This is from the following code</p> <pre><code>APPEND netboot=nfs boot=casper root=/dev/nfs nfsroot=192.168.100.1:/srv/isoubuntu autoinstall \n</code></pre> <p>Copy the files created to the folder </p> <p><code>/srv/autoinstall/</code> on the <code>alpine1</code> server. The duplicate file with the node name is workaround to show the hostname of the specific file. As I did not find away to identify the hostname using the Ubuntu autoinstall method.</p> <p></p> <p>If you recall from 104-setup-nginx, the folder /srv/ was exposed to be visible from <code>http://192.168.100.1/</code>. All folders created in <code>/srv/</code> will be listed via the browser.</p> <p>Copy the files using WSFTP or some other method . </p> <p>Validate by browsing <code>http://192.168.100.1/</code></p> <p></p> <p>// To update Video of Script generation and the Ubuntu setup.</p>"},{"location":"kubernetes-lab-setup/100-alpine1/manual-steps/113-generate-user-data-multipleVM/#lessons-learnt","title":"Lessons Learnt","text":"<p>The MAC addresses for a newly created Hyper-V VM is not generated until its started at least once. </p>"},{"location":"kubernetes-lab-setup/200-kubernetes/200-ReadMe/","title":"Lightweight Kubernetes Setup","text":"<p>If you had completed the steps in <code>100-alpine1</code> then you are ready to Install Lightweight Kubernetes. (k3s)</p> <p>Instead of installing k3s  manualy , I looked for a solution that can help expedite this process. </p> <p>Ansible was quite easy to pickup.  Ansible is not explained in detail here, but if you follow the steps, then you will be able to execute them without issues.</p>"},{"location":"kubernetes-lab-setup/200-kubernetes/200-ReadMe/#summary-of-the-steps","title":"Summary of the steps.","text":"<ol> <li>ssh-keys </li> </ol> <p>ssh-keys are required to allow for automated installation, without this you will have to manualy key in your password or passphrase multiple times.</p> <p>The concept of ssh-keys is simple, 2 ssh-keys are created 1 Private Key and another a Public key. </p> <p>The Public key will need to be stored on the remote servers associated with a specific user.  Once this is done, when you connect from a central server that hosts the private ssh-key , the handshake authorizes the session without any password. </p> <p>You can and must set up a complex  <code>passphrase</code> on the private ssh-key. This is important as if another user manages to get your private ssh-key they cannot use it without the passphrase.</p> <ol> <li>Ansible play books.</li> </ol> <p>Playbooks, contain the specific steps on the target server. This repository has working playbooks to setup the k3s clusters. </p>"},{"location":"kubernetes-lab-setup/200-kubernetes/201-SSH-keys-setup/","title":"SSH keys","text":"<p>When accessing Ubuntu ( or any Linux) servers, you will need to enter the username and password for every SSH session. When performing automated tasks, this can be a challenge.  SSH keys  can be used to improve this process. For our next tool Ansible ssh keys are required.</p> <p>How SSH keys work?</p> <p>The concept of SSH keys involve private and public keys. </p> <p>Private keys </p> <ul> <li>are meant to be kept a secret not shared</li> <li>best to put a complex  password on the private key. </li> <li>are stored in the <code>/.ssh/</code> folder from where the ssh client is accessed.</li> </ul> <p>Public keys </p> <ul> <li>are meant to be shared and are usually copied to the servers that are usually accessed.</li> </ul> <p>run the command <code>ssh-keygen</code> on <code>alpine1</code>. </p> <p>This will create 2 files , <code>ubuntu-k3s-sshkey.pub</code> -public key  and <code>ubuntu-k3s-sshkey</code> private key . The filename can be set at the prompt. Ensure the full path is entered <code>/root/.ssh/ubuntu-k3s-sshkey</code></p> <pre><code>alpine1:~# ssh-keygen\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/root/.ssh/id_rsa): /root/.ssh/ubuntu-k3s-sshkey\nCreated directory '/root/.ssh'.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /root/.ssh/ubuntu-k3s-sshkey\nYour public key has been saved in /root/.ssh/ubuntu-k3s-sshkey.pub\nThe key fingerprint is:\nSHA256:Od1Wxp2mpTJ5YnLD7NShCJc+Y41YIlNLn2enjGZIxzU root@alpine1\nThe key's randomart image is:\n</code></pre>"},{"location":"kubernetes-lab-setup/200-kubernetes/201-SSH-keys-setup/#copy-ssh-keys-to-remote-servers","title":"Copy ssh keys to remote servers.","text":"<p>Once the key pair is generated, copy the public key to the remote Ubuntu server</p> <p>The following script copies the ssh public key to the remote servers  <code>master1 master2 master3 worker1 worker2 worker3 xsinglenode</code></p> <pre><code>#!/bin/sh\n\n# SSH key file , the public key has the `.pub' extension.\nssh_key=\"/root/.ssh/ubuntu-k3s-sshkey.pub\"\n\n# Perform ssh-keyscan for each server and copy SSH public key\nfor server in loadbalancer master1 master2 master3 worker1 worker2 worker3 xsinglenode\ndo\n    # Perform ssh-keyscan to gather SSH host key\n    ssh-keyscan \"$server\" &gt;&gt; ~/.ssh/known_hosts\n\n    # Copy SSH public key to the server\n    # take note the ssh key is associated to the use 'ubuntu' in this example\n    ssh-copy-id -i \"$ssh_key\" \"ubuntu@$server\"  \ndone\n</code></pre>"},{"location":"kubernetes-lab-setup/200-kubernetes/201-SSH-keys-setup/#passphrase","title":"Passphrase","text":"<p>When connecting using ssh keys, the prompt for the ssh key passphrase will be prompted for each ssh session. You can use the ssh-agent to store this passphrase</p> <pre><code># start ssh-agent. \neval $(ssh-agent) \n\n# add the private key using ssh-add, the passphrase will be prompted and not required to manually enter \nssh-add /root/.ssh/ubuntu-k3s-sshkey\n</code></pre> <p>The following script adds passphrase to ssh-agent for use by ansible when connecting to remote servers</p> <p>add-passphrase.sh</p> <pre><code>#kill all ssh-agent currently running\nkillall ssh-agent\n\n# check if ssh-agent is running. \neval $(ssh-agent) \n\n# add the private key using ssh-add, the passphrase will be prompted and not required to manually enter \nssh-add /root/.ssh/ubuntu-k3s-sshkey\n\n# list keys in memory\nssh-add -l\n</code></pre> <p>Troubleshooting points</p> <ol> <li>Ensure only one instance of <code>ssh-agent</code> is running. You can run the following to terminate all <code>ssh-agent</code> and start a new single instance.</li> </ol> <p><code>killall ssh-agent</code></p> <ol> <li>Check existing ssh-agent that are running. <code>ps aux | grep ssh</code></li> </ol>"},{"location":"kubernetes-lab-setup/200-kubernetes/202-Setup-k3s-with-Ansible/","title":"Setup k3s with Ansible","text":"<p>Setting up a single node k3s is very quick , takes about 2-3 minutes to setup. Setting up a k3s High Availability cluster is more complex.</p> <p>Having the goal of getting a Kubernetes Lab setup quickly, the best option I had was to use Ansible.  I have playbooks to setup one k3s High Availability Clusters and a single node k3s. Managing the k3s is best to be remote, to accommodate that, made the <code>alpine1</code> the kubectl management server. </p> <p>The most challenging task I faced when I started with kubernetes was the KUBECONFIG . Once I had this working learning the other tasks were that much easier. The Playbook and bash scripts make this easier for you.  </p> <p>The following describes the fastest way to setup all the servers with the required modules and k3s.</p> <pre><code>execute the playbook run from `/srv/ansible/playbook-k3s/`\nansible-playbook k3s-complete-setup.yml  --user ubuntu --ask-become-pass  -i hosts.ini\n</code></pre> <p>Once the steps are completed , <code>alpine1</code> can be configured to connect to the <code>k3s</code> clusters. </p>"},{"location":"kubernetes-lab-setup/200-kubernetes/202-Setup-k3s-with-Ansible/#kubeconfig-on-alpine1","title":"Kubeconfig on <code>alpine1</code>","text":"<p>Once the scripts <code>02z-k3s-copy-kubeconfig.yml</code> and <code>03z-k3s-copy-kubeconfig-single.yml</code> are completed. The playbook copy the remote kubeconfig files from the remote servers . The files will be stored in <code>/root/.kube/clusters</code>  folder. </p> <p>We now need to have the KUBECONFIG setup on <code>alpine</code> . The following will merge all <code>*.yaml</code> files in <code>/root/.kube/clusters</code></p> <pre><code> export KUBECONFIG=$(for YAML in $(find ${HOME}/.kube/clusters -name '*.yaml') ; do echo -n \":${YAML}\"; done)\n</code></pre> <p>This is not permanent . Do check how you can make this permanent after logout from the session. </p>"},{"location":"kubernetes-lab-setup/200-kubernetes/202-Setup-k3s-with-Ansible/#kubectl-basic-commands","title":"<code>kubectl</code> basic commands","text":"<p>Running some basic <code>kubectl</code> commands.</p> <p>List of contexts. </p> <pre><code>(1) kubectl config get-contexts.\n</code></pre> <p>Change context</p> <pre><code>(2) and (4) kubectl config use-context &lt;context name&gt;\n</code></pre> <p>List kubernetes nodes</p> <pre><code>(3) and (4) kubectl get nodes\n</code></pre> <p>Here is a screenshot that will provide some perspective</p> <p></p>"},{"location":"kubernetes-lab-setup/200-kubernetes/204-k3s-setup-complete/","title":"k3s Setup completed.","text":"<p>Congratulations you now have 2 Lightweight Clusters you can use for your Lab. </p> <p>What's next?</p> <p>You can now attempt to perform the various tasks you want to experiment . </p>"},{"location":"kubernetes-lab-setup/200-kubernetes/manual-steps/202-Ansible-inventory/","title":"Ansible Inventory","text":"<p>The main element for Ansible is the inventory. The inventory stores the list of servers that Ansible can perform actions on. </p> <p>The inventory can be in ini file or yaml  file. </p> <p>In this example we will use the inventory as an <code>ini</code> file.</p> <p>Content of <code>hosts.ini</code> are a follows. </p> <p>To create the ansible folder <code>mkdir /srv/ansible &amp;&amp; cd /srv/ansible</code></p> <p>Create the file /srv/ansible/hosts.ini </p> <p>hosts.ini</p> <p>```ansible hosts.ini [all:vars] ansible_user=ubuntu ansible_ssh_private_key_file=/root/.ssh/ubuntu-k3s-sshkey ansible_ssh_common_args=-o StrictHostKeyChecking=accept-new -o ForwardAgent=yes k3s_cluster_name=k3s-cluster-77 k3s_single_name=k3s-single-01</p> <p>[lb] loadbalancer ansible_host=192.168.100.201</p> <p>[master] master1 ansible_host=192.168.100.202</p> <p>[masterHA] master2 ansible_host=192.168.100.203 master3 ansible_host=192.168.100.204</p> <p>[worker] worker1 ansible_host=192.168.100.205 worker2 ansible_host=192.168.100.206 worker3 ansible_host=192.168.100.207</p> <p>[single] xsinglenode ansible_host=192.168.100.199</p> <p>[k3s_cluster:children] lb master masterHA worker</p> <p>```</p>"},{"location":"kubernetes-lab-setup/200-kubernetes/manual-steps/202-Ansible-inventory/#ansible-playbooks","title":"Ansible Playbooks","text":"<p>Ansible performs specific actions that are listed in a playbook. The Playbook specifies the server to execute the tasks. Each task will perform a specific function. The actual workings of Ansible is outside of the scope of this topic. But the relevant Playbooks and the steps to execute will be shared</p>"},{"location":"kubernetes-lab-setup/200-kubernetes/manual-steps/203-Setup-k3s-with-Ansible/","title":"Ansible Playbook steps","text":"<p>Setting up a single node k3s is very quick , takes about 2-3 minutes to setup. Setting up a k3s High Availability cluster is more complex.</p> <p>Having the goal of getting a Kubernetes Lab setup quickly, the best option I had was to use Ansible.  I have playbooks to setup one k3s High Availability Clusters and a single node k3s. Managing the k3s is best to be remote, to accommodate that, made the <code>alpine1</code> the kubectl management server. </p> <p>The most challenging task I faced when I started with kubernetes was the KUBECONFIG . Once I had this working learning the other tasks were that much easier. The Playbook and bash scripts make this easier for you.  </p> <p>The following describes the relevant files in the <code>ansible</code> folder. That will help with the setup of the 2 k3s clusters mentioned.</p>"},{"location":"kubernetes-lab-setup/200-kubernetes/manual-steps/203-Setup-k3s-with-Ansible/#ansible-folder-content","title":"Ansible folder content","text":"<p>Folder <code>ansible</code></p> <p>Prepare / run this first </p> <p>hosts.ini</p> <ul> <li>contains the list of servers and the various roles they have.</li> <li>The Ansible playbook references this file on the servers to perform the tasks on.</li> </ul> <p>add-passphrase.sh</p> <ul> <li>The script to store the ssh-key passphrase so it can be referenced when Ansible connects to the remote servers.</li> <li>Run this script once before running the playbooks.</li> </ul> <p>copy-ssh-key.sh</p> <ul> <li>The ssh public key must be copied to the remote servers once. </li> <li>This script automates the task .</li> </ul> <p>Folder <code>playbook</code></p> <p>Run the ansible playbook in the sequence listed. The description of the tasks performed are listed below. </p> <p>00-apt-update.yml</p> <ul> <li>Performs apt-update on all Ubuntu servers</li> <li>Installs kubectl on <code>alpine1</code></li> </ul> <pre><code>execute the playbook run from `/srv/ansible`\nansible-playbook 00-apt-update.yml  --user ubuntu --ask-become-pass  -i hosts.ini\n</code></pre> <p>01-k3s-loadbalancer-setup.yml</p> <ul> <li>Installs Docker  and Nginx on the Loadbalancer</li> </ul> <pre><code>execute the playbook run from `/srv/ansible`\nansible-playbook 01-k3s-loadbalancer-setup.yml --user ubuntu --ask-become-pass  -i hosts.ini\n</code></pre> <p>j2\\nginx.conf.j2</p> <ul> <li>When the load balancer is setup with <code>nginx</code> the required configuration for the loadbalancer is generated from the Jinja2 (j2) script.</li> </ul> <p>02-k3s-HA-setup.yml</p> <ul> <li>Installs k3s server on master1,master2 and master3</li> <li>Installs k3s agent on worker1,worker2,worker3.</li> </ul> <pre><code>execute the playbook run from `/srv/ansible`\nansible-playbook 02-k3s-HA-setup.yml --user ubuntu --ask-become-pass  -i hosts.ini\n</code></pre> <p>02z-k3s-copy-kubeconfig.yml</p> <ul> <li>Copies kubeconfig files from cluster to  <code>ansible1</code> in the folder <code>/root/.kube./clusters/</code> to remotely manage the k3s cluster</li> <li>The kubeconfig file also updates the names to be specific to the cluster.</li> <li>Updates the localhost entries to <code>Loadbalancer</code> </li> </ul> <pre><code>execute the playbook run from `/srv/ansible`\nansible-playbook 02z-k3s-copy-kubeconfig.yml --user ubuntu --ask-become-pass  -i hosts.ini\n</code></pre> <p>03-k3s-single-node-setup.yml</p> <ul> <li>Installs k3s as a single instance.</li> </ul> <pre><code>execute the playbook run from `/srv/ansible`\nansible-playbook 03-k3s-single-node-setup.yml --user ubuntu --ask-become-pass  -i hosts.ini\n</code></pre> <p>03z-k3s-copy-kubeconfig-single.yml</p> <ul> <li>Copies kubeconfig files from the single node k3s to  <code>ansible1</code>  in the folder <code>/root/.kube./clusters/</code> to remotely manage the k3s cluster</li> <li>The kubeconfig file also updates the names to be specific to the cluster.</li> <li>Updates the localhost entries to <code>Loadbalancer</code> </li> </ul> <pre><code>execute the playbook run from `/srv/ansible`\nansible-playbook 03z-k3s-copy-kubeconfig-single.yml --user ubuntu --ask-become-pass  -i hosts.ini\n</code></pre> <p>98-k3s-remove-all.yml</p> <ul> <li>If there is a need to remove the k3s installations this playbook removes the k3s from  <code>master1,master2 ,master3,worker1,worker2,worker3,xsinglenode</code></li> </ul> <pre><code>execute the playbook run from `/srv/ansible`\nansible-playbook 98-k3s-remove-all.yml --user ubuntu --ask-become-pass  -i hosts.ini\n</code></pre> <p>ansible-vars.yml</p> <ul> <li>The ansible variables are stored in this file. </li> <li>In this example the file stores the various setup commands to install `k3s' for the different roles. </li> </ul>"},{"location":"kubernetes-lab-setup/200-kubernetes/manual-steps/203-Setup-k3s-with-Ansible/#kubeconfig-on-alpine1","title":"Kubeconfig on <code>alpine1</code>","text":"<p>Once the scripts <code>02z-k3s-copy-kubeconfig.yml</code> and <code>03z-k3s-copy-kubeconfig-single.yml</code> are completed. The playbook copy the remote kubeconfig files from the remote servers . The files will be stored in <code>/root/.kube/clusters</code>  folder. </p> <p>We now need to have the KUBECONFIG setup on <code>alpine</code> . The following will merge all <code>*.yaml</code> files in <code>/root/.kube/clusters</code></p> <pre><code> export KUBECONFIG=$(for YAML in $(find ${HOME}/.kube/clusters -name '*.yaml') ; do echo -n \":${YAML}\"; done)\n</code></pre> <p>This is not permanent . Do check how you can make this permanent after logout from the session. </p>"},{"location":"kubernetes-lab-setup/200-kubernetes/manual-steps/203-Setup-k3s-with-Ansible/#kubectl-basic-commands","title":"<code>kubectl</code> basic commands","text":"<p>Running some basic <code>kubectl</code> commands.</p> <p>List of contexts. </p> <pre><code>(1) kubectl config get-contexts.\n</code></pre> <p>Change context</p> <pre><code>(2) and (4) kubectl config use-context &lt;context name&gt;\n</code></pre> <p>List kubernetes nodes</p> <pre><code>(3) and (4) kubectl get nodes\n</code></pre> <p>Here is a screenshot that will provide some perspective</p> <p></p>"},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/","title":"<code>alpine1</code> setup and configuration.","text":""},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/#setup-boot-files-for-pxelinux-and-os-boot","title":"Setup boot files for <code>PXELINUX</code> and <code>OS boot</code>.","text":"<p>When a new computer starts up in the network it needs a set of boot files that provide instructions.  In this lab setup ( an also production setups) we want to present a boot menu. With this menu, we can then have options to perform multiple tasks. In this case we can have flavours of Operating Systems. Including MS Windows OS deployment. This lab will review the Ubuntu OS deployment. This </p> <p>There are 2 sets of boot files required, <code>PXELINUX boot files</code> and <code>Remote client OS Boot files</code></p>"},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/#pxelinux-boot-files","title":"PXELINUX boot files","text":"<p>It is a good time to highlight there are 2 types of boot process,<code>BIOS</code> and <code>EFI</code>. <code>BIOS</code> is for older Computers. The newer computers work with <code>EFI</code>. This was confusing initialy as the <code>SYSLINUX</code> files actually have versions specific to <code>BIOS</code> , <code>EFI32</code> and <code>EFI64</code>.  In <code>Hyper-V</code> when you create a VM you are given a choice of <code>Generation 1</code> or <code>Generation 2</code>.  If you selected <code>Generation 1</code> then you need to use the <code>BIOS</code> version. In this lab <code>Generation 2</code> was selected when the VM's are created. The BIOS version is stated here to provide this fact that could have saved me a lot of time. The <code>BIOS</code> version is not tested.</p> <p>This boot file is delivered via tftp. Lets review the files required.</p>"},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/#bios-core-files","title":"BIOS Core files","text":"<p><pre><code>pxelinux.0\nldlinux.c32\n</code></pre> Basic Menu These files are required if you want to have a menu selection. If you don't have multple Boot options to present these files are not needed. <pre><code>menu.c32\nlibutil.c32\n</code></pre> Graphics Menu These files are required if you want to have a menu selection but with better graphics where you can have png file as a backgroud. If you don't have multple Boot options to present these files are not needed.</p> <pre><code>vesamenu.c32\nlibcom32.c32\n</code></pre>"},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/#efi64-core-files","title":"EFI64 Core files","text":"<p><pre><code>ldlinux.e64\nsyslinux.efi\n</code></pre> Basic Menu These files are required if you want to have a menu selection. If you don't have multple Boot options to present these files are not needed. <pre><code>menu.c32\nlibutil.c32\n</code></pre> Graphics Menu These files are required if you want to have a menu selection but with better graphics where you can have png file as a backgroud. If you don't have multple Boot options to present these files are not needed.</p> <pre><code>vesamenu.c32\nlibcom32.c32\n</code></pre>"},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/#where-to-get-these-files","title":"Where to get these files?","text":"<p>The files can be downloaded from SYSLINUX ver 6.03 The SYSLINUX has many files, in this lab we will extract the relevant files required. This will be shared in Part 2.</p>"},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/#remote-client-os-boot-files","title":"Remote client OS boot files.","text":"<p>The next set of Boot files are the OS boot files. For example these can be Ubuntu, Debian, Microsoft Windows boot files. In this example we will extract the boot files from the Ubuntu ISO LiveCD. For Linux the boot files required are.  <pre><code>vmlinuz\ninitrd\n</code></pre> The file name can also include the version is some cases, but the base file names will still be present.</p>"},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/#completion-of-boot-file-background","title":"Completion of Boot file background.","text":"<p>Some background was neccasary before proceeding with the boot file preparation. Hopefully provides more clarity.</p>"},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/#next-step","title":"Next step","text":"<p>We will proceed with the preparation of the boot files. Please continue with </p>"},{"location":"kubernetes-lab-setup/900-Explainers/PXELINUX/#110-alpine1-setup-boot-files-part2-pxe","title":"110-alpine1-setup-boot-files-part2-PXE","text":""},{"location":"kubernetes-lab-setup/900-Explainers/generate-user-data-multipleVM/","title":"generate user data multipleVM","text":""},{"location":"kubernetes-lab-setup/900-Explainers/generate-user-data-multipleVM/#generate-user-data-mac-address-file","title":"Generate <code>user-data-mac-address</code> file.","text":"<p>For a Kubernetes cluster we will need multiple Virtual Machines. This will require multiple <code>user-data-mac-address</code> files to be created. Here I use PowerShell to generate the files.</p> <p>To setup each Hyper-V VM with the Ubuntu Operating system we need to generate unique <code>user-data</code> files. The following script generates the script based on a working <code>user-data</code> file. This working file was extracted by installing Ubuntu from scratch with the relevant configurations. This file can be obtained from <code>/var/log/installer/autoinstall-user-data</code>. This file is renamed as <code>user-data-template.yaml</code> in this script. The script then creates new files based on the mac-address with the hostname and IP address updated.  </p> <pre><code># Get the directory containing the script\n$scriptDirectory = Split-Path -Parent $MyInvocation.MyCommand.Path\n\n# Set the path to the script file\nSet-Location $scriptDirectory\n$output = \"$scriptDirectory\\autoinstall\"\nif ((Test-Path -Path $output) -ne \"True\") { New-Item -Path $output -ItemType Directory }\n\n# Define a hashtable for mapping hostnames to IP addresses\n$ipAddressMap = @{\n    \"alpine1\"       = \"192.168.100.1\"\n    \"loadbalancer\"  = \"192.168.100.201\"\n    \"master1\"       = \"192.168.100.202\"\n    \"master2\"       = \"192.168.100.203\"\n    \"master3\"       = \"192.168.100.204\"\n    \"worker1\"       = \"192.168.100.205\"\n    \"worker2\"       = \"192.168.100.206\"\n    \"worker3\"       = \"192.168.100.207\"\n    \"xsinglenode\"   = \"192.168.100.199\"\n}\n\n# Get all virtual machines\n$vms = Get-VM\n\n# Iterate through each virtual machine\nforeach ($vm in $vms) {\n    # Get MAC address of the first network adapter\n    $macAddress = $vm | Get-VMNetworkAdapter | Select-Object -First 1 | Select-Object -ExpandProperty MacAddress\n\n    # Remove \"k8s-\" prefix from the VM name\n    $vmName = $vm.Name -replace '^k8s-', ''\n\n    # Format MAC address\n    $formattedMacAddress = ($macAddress -replace '(.{2})(.{2})(.{2})(.{2})(.{2})(.{2})', '$1-$2-$3-$4-$5-$6').ToLower()\n\n    # Get IP address from the mapping\n    $ipAddress = $ipAddressMap[$vmName]\n\n    if (-not $ipAddress) {\n        Write-Host \"Error: IP address not found for '$vmName'.\"\n        continue\n    }\n\n    # Format DHCP configuration\n    $dhcpConfig = @\"\nhost $vmName {\n    hardware ethernet $formattedMacAddress;\n    fixed-address $ipAddress;\n    option host-name \"$vmName\";\n}\n\"@\n    Write-Host \"----------------------\"\n    # Output results\n   # Write-Host \"VM Name: $vmName\"\n   # Write-Host \"MAC Address: $formattedMacAddress\"\n    Write-Host \"IP Address: $ipAddress\"\n   # Write-Host \"Server Name: $($env:COMPUTERNAME)\"\n\n\n    # Write to the CSV file\n    $mac = \" $vmName,$formattedMacAddress,$ipAddress\"\n    Write-Host $mac\n    $mac | Out-File $output\\mac-address-list.csv -Append\n\n    # Create user-data files\n    # Create user-data files\n    $userDataFile = \"$output\\user-data-$formattedMacAddress\"\n\n    if (-not (Test-Path $userDataFile)) {\n        # File doesn't exist, create it by copying the template\n        Copy-Item -Path \"user-data-template.yaml\" -Destination $userDataFile\n        Write-Host \"File created: $userDataFile\"\n    }\n    #this following file is to be able to identify the mac address with hostname directly from the filename\n    if (-not (Test-Path $userdatafile-$vmName)) {\n    # File doesn't exist, create it\n    New-Item -ItemType File -Path $userdatafile-$vmName \n    Write-Host \"File created: $userdatafile-$vmName\"\n} \n    # Get content of the template file\n    $userDataContent = Get-Content -Path $userDataFile -Raw\n\n    # Replace the placeholders with predetermined values\n    $userDataContent = $userDataContent -replace 'hostname: master1', \"hostname: $vmName\"\n    $userDataContent = $userDataContent -replace '192.168.100.202/24', \"$ipAddress/24\"\n\n    # Set the modified content back to the user-data file\n    Set-Content -Path $userDataFile -Value $userDataContent\n    Write-Host \"----------------------\"\n\n}\n</code></pre> <p>This script references 2 different files <code>user-data-early</code> and another for <code>user-data-template</code>.</p> <p>These 2 files have some differences. </p> <p><code>user-data</code> </p> <ul> <li> <p>On row 2 contains the entry <code>autoinstall:</code> </p> </li> <li> <p>On row 28 and 29 contains the early command . This is when the file <code>user-data-mac-address</code> is downloaded to <code>autoinstall.yaml</code></p> </li> </ul> <pre><code>  early-commands:\n    - curl -G -o /autoinstall.yaml http://192.168.100.1/autoinstall/user-data-\"$(ip a | grep ether | awk '{print $2}' | tr ':' '-')\"\n</code></pre> <p><code>user-data-template</code></p> <ul> <li>Does not have the  <code>autoinstall:</code>  entry. Having this entry will cause the installation to fail.</li> <li>Does not have the early command to download the <code>user-data-template</code> as this was already done.</li> </ul> <p>The generated files are in the subfolder  <code>.\\autoinstall\\</code></p> <p></p> <p>From the configuration in step 112 , the Boot sequence is looking up the location. <code>http://192.168.100.1/autoinstall/</code></p> <p>This is from the following code</p> <pre><code>APPEND netboot=nfs boot=casper root=/dev/nfs nfsroot=192.168.100.1:/srv/isoubuntu autoinstall \n</code></pre> <p>Copy the files created to the folder </p> <p><code>/srv/autoinstall/</code> on the <code>alpine1</code> server. The duplicate file with the node name is workaround to show the hostname of the specific file. As I did not find away to identify the hostname using the Ubuntu autoinstall method.</p> <p></p> <p>If you recall from 104-setup-nginx, the folder /srv/ was exposed to be visible from <code>http://192.168.100.1/</code>. All folders created in <code>/srv/</code> will be listed via the browser.</p> <p>Copy the files using WSFTP or some other method . </p> <p>Validate by browsing <code>http://192.168.100.1/</code></p> <p></p> <p>To update Video of Script generation and the Ubuntu setup.</p> <p>, <code>vendor-data</code> </p> <p>Troubleshooting tips</p> <p>The MAC addresses for a newly created HyperV VM is not generated until its started at least once. </p>"},{"location":"kubernetes-lab-setup/900-Explainers/generate-user-data-reference/","title":"Generate user data reference","text":""},{"location":"kubernetes-lab-setup/900-Explainers/generate-user-data-reference/#gotchas","title":"Gotchas","text":"<p>In preparing the autoinstall I had a few mistakes I made and identifying them did take alot of time. </p> <ol> <li>Ensure Virtual Machine specifications meet the minimum requirements.</li> </ol> <p>I had set the Memory to 512MB and enabled Dynamic memory of 512MB to 2048GB but this caused the autoinstall to crash and not complete.</p> <ol> <li>The second issue I faced was to use the incorrect entries for <code>user-data-early</code> and <code>user-data-template.yaml</code> . Please refer to the content I used below.</li> </ol> <p><code>user-data-early</code> content.</p> <pre><code>#cloud-config\nautoinstall:\n  apt:\n    disable_components: []\n    geoip: true\n    preserve_sources_list: false\n    primary:\n    - arches:\n      - amd64\n      - i386\n      uri: http://my.archive.ubuntu.com/ubuntu\n    - arches:\n      - default\n      uri: http://ports.ubuntu.com/ubuntu-ports\n  drivers:\n    install: false\n  identity:\n    hostname: master1\n    password: $5$1hs/5SKiGm.zNbWk$Ap5W6Tc.YCaALtSp1INrLmYD/GIpRemhpRwtZIZCSO9\n    realname: ss\n    username: ubuntu\n  kernel:\n    package: linux-generic\n  keyboard:\n    layout: us\n    toggle: null\n    variant: ''\n  early-commands:\n    - curl -G -o /autoinstall.yaml http://192.168.100.1/autoinstall/user-data-\"$(ip a | grep ether | awk '{print $2}' | tr ':' '-')\"\n  locale: en_US.UTF-8\n  network:\n    ethernets:\n      eth0:\n        addresses:\n        - 192.168.100.201/24\n        gateway4: 192.168.100.1\n        nameservers:\n          addresses:\n          - 192.168.100.1\n          - 8.8.8.8\n          search: [k8s.lab]\n    version: 2\n  ssh:\n    allow-pw: true\n    authorized-keys: []\n    install-server: true\n  storage:\n    config:\n    - ptable: gpt\n      path: /dev/sda\n      wipe: superblock\n      preserve: false\n      name: ''\n      grub_device: false\n      type: disk\n      id: disk-sda\n    - device: disk-sda\n      size: 1127219200\n      wipe: superblock\n      flag: boot\n      number: 1\n      preserve: false\n      grub_device: true\n      type: partition\n      id: partition-0\n    - fstype: fat32\n      volume: partition-0\n      preserve: false\n      type: format\n      id: format-0\n    - device: disk-sda\n      size: 2147483648\n      wipe: superblock\n      flag: ''\n      number: 2\n      preserve: false\n      grub_device: false\n      type: partition\n      id: partition-1\n    - fstype: ext4\n      volume: partition-1\n      preserve: false\n      type: format\n      id: format-1\n    - device: disk-sda\n      size: 50410291200\n      wipe: superblock\n      flag: ''\n      number: 3\n      preserve: false\n      grub_device: false\n      type: partition\n      id: partition-2\n    - name: ubuntu-vg\n      devices:\n      - partition-2\n      preserve: false\n      type: lvm_volgroup\n      id: lvm_volgroup-0\n    - name: ubuntu-lv\n      volgroup: lvm_volgroup-0\n      size: 25203572736B\n      wipe: superblock\n      preserve: false\n      type: lvm_partition\n      id: lvm_partition-0\n    - fstype: ext4\n      volume: lvm_partition-0\n      preserve: false\n      type: format\n      id: format-2\n    - path: /\n      device: format-2\n      type: mount\n      id: mount-2\n    - path: /boot\n      device: format-1\n      type: mount\n      id: mount-1\n    - path: /boot/efi\n      device: format-0\n      type: mount\n      id: mount-0\n  version: 1\n</code></pre> <p>user-data-template.yaml</p> <pre><code>#cloud-config\n  apt:\n    disable_components: []\n    geoip: true\n    preserve_sources_list: false\n    primary:\n    - arches:\n      - amd64\n      - i386\n      uri: http://my.archive.ubuntu.com/ubuntu\n    - arches:\n      - default\n      uri: http://ports.ubuntu.com/ubuntu-ports\n  drivers:\n    install: false\n  identity:\n    hostname: master1\n    password: $5$1hs/5SKiGm.zNbWk$Ap5W6Tc.YCaALtSp1INrLmYD/GIpRemhpRwtZIZCSO9\n    realname: ss\n    username: ubuntu\n  kernel:\n    package: linux-generic\n  keyboard:\n    layout: us\n    toggle: null\n    variant: ''\n  locale: en_US.UTF-8\n  network:\n    ethernets:\n      eth0:\n        addresses:\n        - 192.168.100.202/24\n        gateway4: 192.168.100.1\n        nameservers:\n          addresses:\n          - 192.168.100.1\n          - 8.8.8.8\n          search: [k8s.lab]\n    version: 2\n  ssh:\n    allow-pw: true\n    authorized-keys: []\n    install-server: true\n  storage:\n    config:\n    - ptable: gpt\n      path: /dev/sda\n      wipe: superblock\n      preserve: false\n      name: ''\n      grub_device: false\n      type: disk\n      id: disk-sda\n    - device: disk-sda\n      size: 1127219200\n      wipe: superblock\n      flag: boot\n      number: 1\n      preserve: false\n      grub_device: true\n      type: partition\n      id: partition-0\n    - fstype: fat32\n      volume: partition-0\n      preserve: false\n      type: format\n      id: format-0\n    - device: disk-sda\n      size: 2147483648\n      wipe: superblock\n      flag: ''\n      number: 2\n      preserve: false\n      grub_device: false\n      type: partition\n      id: partition-1\n    - fstype: ext4\n      volume: partition-1\n      preserve: false\n      type: format\n      id: format-1\n    - device: disk-sda\n      size: 50410291200\n      wipe: superblock\n      flag: ''\n      number: 3\n      preserve: false\n      grub_device: false\n      type: partition\n      id: partition-2\n    - name: ubuntu-vg\n      devices:\n      - partition-2\n      preserve: false\n      type: lvm_volgroup\n      id: lvm_volgroup-0\n    - name: ubuntu-lv\n      volgroup: lvm_volgroup-0\n      size: 25203572736B\n      wipe: superblock\n      preserve: false\n      type: lvm_partition\n      id: lvm_partition-0\n    - fstype: ext4\n      volume: lvm_partition-0\n      preserve: false\n      type: format\n      id: format-2\n    - path: /\n      device: format-2\n      type: mount\n      id: mount-2\n    - path: /boot\n      device: format-1\n      type: mount\n      id: mount-1\n    - path: /boot/efi\n      device: format-0\n      type: mount\n      id: mount-0\n  updates: security \n  version: 1\n</code></pre> <p>Here is a screenshot of the error when the <code>autoinstall</code> entry is not removed from <code>user-data-template.yaml</code></p> <p></p> <p>The setup will continue but will stop, only when you press enter the script will exit.</p> <p></p>"}]}